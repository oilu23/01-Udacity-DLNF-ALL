{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming quiz\n",
    "Below, you'll implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers.\n",
    "\n",
    "Things to do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Calculate the input to the hidden layer.\n",
    "    Calculate the hidden layer output.\n",
    "    Calculate the input to the output layer.\n",
    "    Calculate the output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make some fake data ”inputs”. Return a sample (or samples) from the “standard normal” distribution.\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986] <class 'numpy.ndarray'> (4,)\n",
      "Initialize hidden weights\n",
      "[[-0.02341534 -0.0234137   0.15792128]\n",
      " [ 0.07674347 -0.04694744  0.054256  ]\n",
      " [-0.04634177 -0.04657298  0.02419623]\n",
      " [-0.19132802 -0.17249178 -0.05622875]] <class 'numpy.ndarray'> (4, 3)\n",
      "Initialize output weights\n",
      "[[-0.10128311  0.03142473]\n",
      " [-0.09080241 -0.14123037]\n",
      " [ 0.14656488 -0.02257763]] <class 'numpy.ndarray'> (3, 2)\n",
      "The linear combination of the inputs * random hidden_weights + bias form the input h.\n",
      "[-1.34365494 -1.29801368 -0.99902638]\n",
      "Hidden-layer Output using np.dot\n",
      "Hidden-layer Output:\n",
      "[ 0.20690965  0.2144995   0.26913289]\n",
      "The linear combination of the hidden_layer inputs * random output_weights + bias form the input h.\n",
      "[-1.0009881  -1.02986815]\n",
      "Hidden-layer Output using np.dot\n",
      "Output-layer Output:\n",
      "[ 0.26874719  0.26310967]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4 # number of input layers\n",
    "N_hidden = 3 # number of hidden layers\n",
    "N_output = 2 # number of output layers\n",
    "bias = -1.0\n",
    "\n",
    "# This method is called when RandomState is initialized. It can be called again to re-seed the generator. \n",
    "# For details, see RandomState.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make some fake data ”inputs”. Return a sample (or samples) from the “standard normal” distribution.\n",
    "X = np.random.randn(4)\n",
    "print(\"Make some fake data ”inputs”. Return a sample (or samples) from the “standard normal” distribution.\")\n",
    "print(X, type(X), np.shape(X))\n",
    "\n",
    "# initialize hidden weights providing the shape of the matrix with \n",
    "# the number of input layers and number of hidden layers\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "print(\"Initialize hidden weights\")\n",
    "print(weights_input_to_hidden, type(weights_input_to_hidden), np.shape(weights_input_to_hidden))\n",
    "\n",
    "# initialize output weights providing the shape of the matrix with \n",
    "# the number of hidden layers and number of output layers\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "print(\"Initialize output weights\")\n",
    "print(weights_hidden_to_output, type(weights_hidden_to_output), np.shape(weights_hidden_to_output))\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "\n",
    "# The linear combination of the inputs * random hidden_weights + bias form the input h.\n",
    "hidden_layer_in = bias+(np.dot(X, weights_input_to_hidden))\n",
    "print(\"The linear combination of the inputs * random hidden_weights + bias form the input h.\")\n",
    "print(hidden_layer_in)\n",
    "\n",
    "# h passes through the activation function giving the final output of the perceptron, labeled y. \n",
    "# Hidden-layer Output using np.dot (y-hat)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "print(\"Hidden-layer Output using np.dot\")\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "# The linear combination of the hidden_layer inputs * random output_weights + bias form the input h.\n",
    "output_layer_in = bias+(np.dot(hidden_layer_out, weights_hidden_to_output))\n",
    "print(\"The linear combination of the hidden_layer inputs * random output_weights + bias form the input h.\")\n",
    "print(output_layer_in)\n",
    "\n",
    "# h passes through the activation function giving the final output of the perceptron, labeled y. \n",
    "# Hidden-layer Output using np.dot (y-hat)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "print(\"Hidden-layer Output using np.dot\")\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0]\n",
      "  [1 1]]\n",
      "\n",
      " [[2 2]\n",
      "  [3 3]]] <class 'numpy.ndarray'> (2, 2, 2)\n",
      "0 <class 'int'> ()\n",
      "0.1 <class 'float'> ()\n",
      "[[ 0.04967142 -0.01382643]\n",
      " [ 0.06476885  0.15230299]] <class 'numpy.ndarray'> (2, 2)\n",
      "[[-0.23415337 -0.23413696  1.57921282]\n",
      " [ 0.76743473 -0.46947439  0.54256004]\n",
      " [-0.46341769 -0.46572975  0.24196227]] <class 'numpy.ndarray'> (3, 3)\n",
      "-1.913280244657798 <class 'float'> ()\n",
      "[[-1.72491783 -0.56228753]\n",
      " [-1.01283112  0.31424733]] <class 'numpy.ndarray'> (2, 2)\n",
      "[-0.90802408 -1.4123037   1.46564877 -0.2257763 ] <class 'numpy.ndarray'> (4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test1 = np.array([[[0,0],[1,1]],\n",
    "                 [[2,2],[3,3]]])\n",
    "print(test1, type(test1), np.shape(test1))\n",
    "\n",
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "print(mu, type(mu), np.shape(mu))\n",
    "print(sigma, type(sigma), np.shape(sigma))\n",
    "\n",
    "#s = np.random.normal(mu, sigma, 1000)\n",
    "s1 = np.random.normal(mu, sigma, size=(2,2))\n",
    "s2 = np.random.normal(size=(3,3))\n",
    "s3 = np.random.normal() \n",
    "s4 = np.random.randn(2, 2)\n",
    "s5 = np.random.randn(4)\n",
    "\n",
    "#print(s, type(s), np.shape(s))\n",
    "print(s1, type(s1), np.shape(s1))\n",
    "print(s2, type(s2), np.shape(s2))\n",
    "print(s3, type(s3), np.shape(s3))\n",
    "print(s4, type(s4), np.shape(s4))\n",
    "print(s5, type(s5), np.shape(s5))\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "#hidden_inputs = np.dot(inputs, weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 6)\n",
      "360 6\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "          gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
      "209 -0.066657  0.289305       0       1       0       0\n",
      "280  0.625884  1.445476       0       1       0       0\n",
      "33   1.837832  1.603135       0       0       1       0\n",
      "210  1.318426 -0.131120       0       0       0       1\n",
      "93  -0.066657 -1.208461       0       1       0       0\n",
      "84  -0.759199  0.552071       0       0       1       0\n",
      "329 -0.759199 -1.208461       0       0       0       1\n",
      "94   0.625884  0.131646       0       1       0       0\n",
      "266 -0.239793 -0.393886       0       0       0       1\n",
      "126  0.106478  0.394412       1       0       0       0\n",
      "[ 0.2027827  -0.05644616  0.26441774  0.62177434 -0.09559271 -0.09558601]\n",
      "Train loss:  0.2627609385\n",
      "Train loss:  0.209286194093\n",
      "Train loss:  0.200842929081\n",
      "Train loss:  0.198621564755\n",
      "Train loss:  0.197798513967\n",
      "Train loss:  0.197425779122\n",
      "Train loss:  0.197235077462\n",
      "Train loss:  0.197129456251\n",
      "Train loss:  0.197067663413\n",
      "Train loss:  0.197030058018\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of records and input units\n",
    "n_records, n_features = features.shape\n",
    "print(features.shape)\n",
    "print(n_records, n_features)\n",
    "print(type(features))\n",
    "print(features.head(10))\n",
    "last_loss = None\n",
    "\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "#weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "print(weights)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = np.dot(x, weights)\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - sigmoid(output)\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error * sigmoid_prime(output)\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += 0\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 3\n",
    "b = 13\n",
    "\n",
    "print (a)\n",
    "\n",
    "def hidden_layer_in1(X, w):\n",
    "    \n",
    "    if X==3:\n",
    "        print (\"test\")\n",
    "        print (X)\n",
    "        X = 0\n",
    "        a = 0 \n",
    "        print (X)\n",
    "        #return = np.dot(X,weights_input_to_hidden)\n",
    "        return X\n",
    "    elif w==13:\n",
    "        w = 15\n",
    "        return w\n",
    "    \n",
    "mira = hidden_layer_in1(a, b)\n",
    "print (a)\n",
    "print (mira)\n",
    "\n",
    "mira2 = hidden_layer_in1(a, b)\n",
    "print (hidden_layer_in1(a, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
