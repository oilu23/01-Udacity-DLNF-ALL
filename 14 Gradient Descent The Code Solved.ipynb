{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Gradient Descent: The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From before we saw that one weight update can be calculated as:\n",
    "\n",
    "Δw\n",
    "​i\n",
    "​​ =ηδx\n",
    "​i\n",
    "​​ \n",
    "\n",
    "with the error term δ as\n",
    "\n",
    "δ=(y−\n",
    "​y\n",
    "​^\n",
    "​​ )f\n",
    "​′\n",
    "​​ (h)=(y−\n",
    "​y\n",
    "​^\n",
    "​​ )f\n",
    "​′\n",
    "​​ (∑w\n",
    "​i\n",
    "​​ x\n",
    "​i\n",
    "​​ )\n",
    "\n",
    "Remember, in the above equation (y−\n",
    "​y\n",
    "​^\n",
    "​​ ) is the output error, and f\n",
    "​′\n",
    "​​ (h) refers to the derivative of the activation function, f(h). We'll call that derivative the output gradient.\n",
    "\n",
    "Now I'll write this out in code for the case of only one output unit. We'll also be using the sigmoid as the activation function f(h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "### Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "### Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "### Target\n",
    "y = 0.2\n",
    "### Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "### The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "### the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]\n",
    "### or h = np.dot(x, weights)\n",
    "\n",
    "### The neural network output (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "### output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "### output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "### error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "### Gradient descent step \n",
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]\n",
    "### or del_w = learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Me Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]\n",
    "# or h = np.dot(x, weights)\n",
    "\n",
    "# The neural network output (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]\n",
    "# or del_w = learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Me Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "weight_test = [-0.2] = wi\n",
    "\n",
    "inputs_test = [7] = xi\n",
    "\n",
    "weight_grades = [-1] = wi\n",
    "\n",
    "inputs_grades = [4] = xi\n",
    "\n",
    "__The learning rate, eta in the weight step equation\n",
    "learnrate = [0.5] = η\n",
    "\n",
    "__True Target value\n",
    "y = [0.2] = y\n",
    "\n",
    "(0.5 × ((0.2 - (1/(1+2.71828183^(-0.2×7+-1×4)))) × ((1/(1+2.71828183^(-0.2×7+-1×4))) × (1 - (1/(1+2.71828183^(-0.2×7+-1×4)))))) × 7) = -0.01246251922\n",
    "\n",
    "(-0.2×7+-1×4) = -5.4 = __Linear combination output = ∑ wi × xi__\n",
    "\n",
    "(0.5 × ((0.2 - (1/(1+2.71828183^-5.4))) × ((1/(1+2.71828183^-5.4)) × (1 - (1/(1+2.71828183^-5.4))))) × 7)\n",
    "\n",
    "(1/(1+2.71828183^-5.4)) = 0.9955037268390589 = __Sigmoid function output  f(h)__\n",
    "\n",
    "(0.5 × ((0.2 - 0.9955037268390589) × (0.9955037268390589 × (1 - 0.9955037268390589))) × 7)\n",
    "\n",
    "(0.9955037268390589 × (1 - 0.9955037268390589)) = 0.0044760566886033444 __Derivative of the sigmoid function = f′(h)__\n",
    "\n",
    "(0.5 × ((0.2 - 0.9955037268390589) × 0.0044760566886033444) × 7) \n",
    "\n",
    "(0.2 - 0.9955037268390589) = -0.7955037268390588 = __Output error = E__\n",
    "\n",
    "(0.5 × (-0.7955037268390588 × 0.0044760566886033444) × 7) \n",
    "\n",
    "(-0.7955037268390588 × 0.0044760566886033444) = -0.003560719777326857 = __Error term (lowercase delta) = δ__\n",
    "\n",
    "((0.5 × -0.003560719777326857 × 7), (0.5 × -0.003560719777326857 × 4)) = 0.012462519220644, -0.0071214395546537138 =  __Gradient descent step = ηδxi = Δwi__ \n",
    "\n",
    "((-0.2 + 0.5 × -0.003560719777326857 × 7), (-1 + 0.5 × -0.003560719777326857 × 4)) = -0.21246251922064402, -1.0071214395546537 =  __Gradient descent step with wi = wi+ηδxi = Δwi__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear combination output [-5.4] and Linear combination with bias [-6.4]\n",
      "Heaviside step function output [0]\n",
      "Exponential number is [[ 2.71828183  2.71828183  0.04978707]]\n",
      "Exponential of Linear combination [0.004516580942612666]\n",
      "Exponential of Linear combination [1.0045165809426126]\n",
      "Exponential of Linear combination [0.9955037268390589]\n",
      "Sigmoid function output [0.9955037268390589]\n",
      "Output error (y - y-hat) [-0.7955037268390588]\n",
      "Derivative of the sigmoid function [0.0044760566886033444]\n",
      "Error term (lowercase delta) [-0.003560719777326857]\n",
      "Gradient descent step with out weight_test and weight_grades [-0.012462519220644, -0.0071214395546537138]\n",
      "Gradient descent step with weight_test and weight_grades [-0.21246251922064402, -1.0071214395546537]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weight_test = -0.2\n",
    "weight_grades = -1\n",
    "bias = -1.0\n",
    "inputs_test = 7\n",
    "inputs_grades = 4\n",
    "\n",
    "# Linear combination\n",
    "lc = weight_test*inputs_test + weight_grades*inputs_grades\n",
    "print(\"Linear combination output [{}] and Linear combination with bias [{}]\".format(lc, lc+bias))\n",
    "\n",
    "# Heaviside step function\n",
    "hstep_f = 0\n",
    "if bias + lc < 0:\n",
    "    hstep_f = 0\n",
    "elif bias + lc >= 0:\n",
    "    hstep_f = 1 \n",
    "print(\"Heaviside step function output [{}]\".format(hstep_f))\n",
    "np.exp(lc)\n",
    "\n",
    "# Sigmoid function is the (exponential of Linear combination + 1)/1 = sigmoid\n",
    "print(\"Exponential number is [{}]\".format(np.exp([1,1,-3])))\n",
    "print(\"Exponential of Linear combination [{}]\".format(np.exp(lc)))\n",
    "print(\"Exponential of Linear combination [{}]\".format(1+np.exp(lc)))\n",
    "print(\"Exponential of Linear combination [{}]\".format(1/(1+np.exp(lc))))\n",
    "sigmoid_fun = 1/(1+np.exp(lc))\n",
    "print(\"Sigmoid function output [{}]\".format(sigmoid_fun))\n",
    "\n",
    "# Target\n",
    "y = 0.2\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - sigmoid_fun\n",
    "print(\"Output error (y - y-hat) [{}]\".format(error))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "# output gradient (f'(h))\n",
    "sigmoid_prime = sigmoid_fun * (1 - sigmoid_fun)\n",
    "print(\"Derivative of the sigmoid function [{}]\".format(sigmoid_prime))\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * sigmoid_prime\n",
    "print(\"Error term (lowercase delta) [{}]\".format(error_term))\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# Gradient descent step with out weight_test and weight_grades\n",
    "del_w2 = [ learnrate * error_term * inputs_test,\n",
    "        learnrate * error_term * inputs_grades]\n",
    "print(\"Gradient descent step with out weight_test and weight_grades {}\".format(del_w2))\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = [ weight_test + learnrate * error_term * inputs_test,\n",
    "          weight_grades + learnrate * error_term * inputs_grades]\n",
    "# or del_w = learnrate * error_term * x\n",
    "print(\"Gradient descent step with weight_test and weight_grades {}\".format(del_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Gradient Descent: The Code test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.8\n",
      "Neural Network output:\n",
      "0.689974481128\n",
      "Amount of Error:\n",
      "-0.189974481128\n",
      "Change in Weights:\n",
      "[array([-0.02031869, -0.04063738, -0.06095608, -0.08127477])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consilated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "#for i in range(len(x)):\n",
    "    #h = w[i]*x[i]\n",
    "\n",
    "h = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + w[3]*x[3]\n",
    "print(h)\n",
    "print(np.dot(x, w))\n",
    "\n",
    "#for xhere, where in zip(x, w):\n",
    "   #print(xhere,where)\n",
    "    #h = h + w[0]*x[0]\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * output_grad\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = [ learnrate * error_term * x ]\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
