{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How to remove tab indent from several lines in IDLE?\n",
    "    \n",
    "    Have you tried Shift+Tab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1[0.1] x2[0.3] w1[0.4] w2[-0.2] hw[0.1] y[1]\n",
      "and the output of the hidden unit h\n",
      "-0.01999999999999999\n",
      "the output of the hidden unit a\n",
      "0.49500016666\n",
      "the output of the network is y1\n",
      "0.512372477963\n",
      "the error for the output unit is b\n",
      "0.121832235361\n",
      "the error for the hidden unit error with backpropagation is bh\n",
      "0.00304550132373\n",
      "the gradient descent steps is gdw\n",
      "0.0308996351456\n",
      "input to hidden weights wi is w1,w2\n",
      "0.000156043105988 0.000468129317964\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inputs and weights\n",
    "x1 = 0.1\n",
    "x2 = 0.3\n",
    "w1 = 0.4\n",
    "w2 = -0.2\n",
    "\n",
    "# Hidden weight\n",
    "hw = 0.1\n",
    "\n",
    "#target value\n",
    "y = 1\n",
    "\n",
    "for i in range(1):\n",
    "    print(\"x1[{}] x2[{}] w1[{}] w2[{}] hw[{}] y[{}]\".format(x1,x2,w1,w2,hw,y))\n",
    "\n",
    "    # Assume we're trying to fit some binary data and the target is y=1. \n",
    "    # We'll start with the forward pass, first calculating the input to the hidden unit\n",
    "    # h = ∑iwi​xi = 0.1×0.4−0.2×0.3 = −0.02\n",
    "    h = w1*x1 + w2*x2\n",
    "    print(\"and the output of the hidden unit h\")\n",
    "    print(h)\n",
    "\n",
    "    # Defining the sigmoid function for activations\n",
    "    def sigmoid(x):\n",
    "        \"\"\"\n",
    "        Calculate sigmoid\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    # and the output of the hidden unit\n",
    "    # a = f(h) = sigmoid(−0.02) = 0.495.\n",
    "    a = sigmoid(h)\n",
    "    print(\"the output of the hidden unit a\")\n",
    "    print(a)\n",
    "\n",
    "    # Using this as the input to the output unit, the output of the network is\n",
    "    # y^ = f(W⋅a) = sigmoid(0.1×0.495) = 0.512.\n",
    "    y1 = sigmoid(hw*a)\n",
    "    print(\"the output of the network is y1\")\n",
    "    print(y1)\n",
    "\n",
    "\n",
    "    # With the network output, we can start the backwards pass to calculate the weight updates for both layers. \n",
    "    # Using the fact that for the sigmoid function \n",
    "    # f′(W⋅a) = f(W⋅a)(1−f(W⋅a)), the error for the output unit is\n",
    "    # δo = (y−y^)f′(W⋅a) = (1−0.512)×0.512×(1−0.512) = 0.122.\n",
    "    b = (y-y1)*y1*(y-y1)\n",
    "    print(\"the error for the output unit is b\")\n",
    "    print(b)\n",
    "\n",
    "\n",
    "    # Now we need to calculate the error for the hidden unit with backpropagation. \n",
    "    # Here we'll scale the error from the output unit by the weight W connecting it to the hidden unit. \n",
    "    # For the hidden unit error, δhj = ∑k Wjk δok f′(hj), \n",
    "    # but since we have one hidden unit and one output unit, this is much simpler.\n",
    "    # δh = Wδo f′(h) = 0.1×0.122×0.495×(1−0.495) = 0.003\n",
    "    bh = hw*b*a*(y-a)\n",
    "    print(\"the error for the hidden unit error with backpropagation is bh\")\n",
    "    print(bh)\n",
    "\n",
    "    # Now that we have the errors, we can calculate the gradient descent steps. \n",
    "    # The hidden to output weight step is the learning rate, times the output unit error, \n",
    "    # times the hidden unit activation value.\n",
    "    # ΔW = ηδoa = 0.5×0.122×0.495 = 0.0302\n",
    "    gdw = y1*b*a\n",
    "    print(\"the gradient descent steps is gdw\")\n",
    "    print(gdw)\n",
    "\n",
    "    # Then, for the input to hidden weights wi, it's the learning rate times the hidden unit error, \n",
    "    # times the input values.\n",
    "    # Δwi = ηδhxi = (0.5×0.003×0.1, 0.5×0.003×0.3) = (0.00015,0.00045)\n",
    "    #hw = y1*bh*x1,y1*bh*x2\n",
    "    w1,w2 = y1*bh*x1,y1*bh*x2\n",
    "    print(\"input to hidden weights wi is w1,w2\")\n",
    "    print(w1,w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From this example, you can see one of the effects of using the sigmoid function for the activations. The maximum derivative of the sigmoid function is 0.25, so the errors in the output layer get reduced by at least 75%, and errors in the hidden layer are scaled down by at least 93.75%! You can see that if you have a lot of layers, using a sigmoid activation function will quickly reduce the weight steps to tiny values in layers near the input. This is known as the vanishing gradient problem. Later in the course you'll learn about other activation functions that perform better in this regard and are more commonly used in modern network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Backpropagation exercise\n",
    "Below, you'll implement the code to calculate one backpropagation update step for two sets of weights. I wrote the forward pass, your goal is to code the backward pass.\n",
    "\n",
    "Things to do\n",
    "\n",
    "    Calculate the network error.\n",
    "    Calculate the output layer error gradient.\n",
    "    Use backpropagation to calculate the hidden layer error.\n",
    "    Calculate the weight update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "0.00641673759167\n",
      "the gradient descent steps is gdw\n",
      "[ 0.0017418   0.00120428]\n",
      "Change in weights for hidden layer to output layer:\n",
      "[  9.71659803e-06  -2.01541576e-05]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  9.78615691e-08   1.28793638e-06]\n",
      " [  3.91446276e-09   8.58624255e-08]\n",
      " [ -7.82892553e-09   6.01036978e-07]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate error\n",
    "error = (target-output)*output*(target-output)\n",
    "print(\"error\")\n",
    "print(error)\n",
    "\n",
    "# TODO: Calculate error gradient for output layer\n",
    "del_err_output = weights_hidden_output*error*output*(target-output)\n",
    "\n",
    "# TODO: Calculate error gradient for hidden layer\n",
    "del_err_hidden = weights_input_hidden*del_err_output*hidden_layer_output*(target-hidden_layer_output)\n",
    "\n",
    "gdw = output*error*hidden_layer_output\n",
    "print(\"the gradient descent steps is gdw\")\n",
    "print(gdw)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = output*del_err_output*hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = output*del_err_hidden*x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "error is wrong. error should be 0.11502656915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "0.11502656915\n",
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate error\n",
    "error = target - output\n",
    "print(\"error\")\n",
    "print(error)\n",
    "\n",
    "# TODO: Calculate error gradient for output layer\n",
    "del_err_output = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error gradient for hidden layer\n",
    "del_err_hidden = np.dot(del_err_output, weights_hidden_output) * \\\n",
    "                 hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * del_err_hidden * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
