{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy provides a function that calculates the dot product of two arrays, which conveniently calculates h for us. The dot product multiplies two arrays element-wise, the first element in array 1 is multiplied by the first element in array 2, and so on. Then, each product is summed.\n",
    "\n",
    "    #input to the output layer<br>\n",
    "    output_in = np.dot(weights, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the general algorithm for updating the weights with gradient descent:\n",
    "\n",
    "Set the weight step to zero: Δw\n",
    "​i\n",
    "​​ =0\n",
    "\n",
    "For each record in the training data:\n",
    "\n",
    "Make a forward pass through the network, calculating the output \n",
    "​y\n",
    "​^\n",
    "​​ =f(∑\n",
    "​i\n",
    "​​ w\n",
    "​i\n",
    "​​ x\n",
    "​i\n",
    "​​ )\n",
    "\n",
    "Calculate the error term for the output unit, δ=(y−\n",
    "​y\n",
    "​^\n",
    "​​ )∗f\n",
    "​′\n",
    "​​ (∑\n",
    "​i\n",
    "​​ w\n",
    "​i\n",
    "​​ x\n",
    "​i\n",
    "​​ )\n",
    "\n",
    "Update the weight step Δw\n",
    "​i\n",
    "​​ =Δw\n",
    "​i\n",
    "​​ +δx\n",
    "​i\n",
    "​​ \n",
    "\n",
    "Update the weights w\n",
    "​i\n",
    "​​ =w\n",
    "​i\n",
    "​​ +ηΔw\n",
    "​i\n",
    "​​ /m where η is the learning rate and m is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    "\n",
    "Repeat for e epochs.\n",
    "\n",
    "You can also update the weights on each record instead of averaging the weight steps after going through all the records.\n",
    "\n",
    "Remember that we're using the sigmoid for the activation function, f(h)=1/(1+e\n",
    "​−h\n",
    "​​ )\n",
    "\n",
    "And the gradient of the sigmoid is f\n",
    "​′\n",
    "​​ (h)=f(h)(1−f(h))\n",
    "\n",
    "where h is the input to the output unit,\n",
    "\n",
    "h=∑\n",
    "​i\n",
    "​​ w\n",
    "​i\n",
    "​​ x\n",
    "​i\n",
    "​​ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2027827  -0.05644616  0.26441774  0.62177434 -0.09559271 -0.09558601]\n",
      "Train loss:  0.2627609385\n",
      "Train loss:  0.209286194093\n",
      "Train loss:  0.200842929081\n",
      "Train loss:  0.198621564755\n",
      "Train loss:  0.197798513967\n",
      "Train loss:  0.197425779122\n",
      "Train loss:  0.197235077462\n",
      "Train loss:  0.197129456251\n",
      "Train loss:  0.197067663413\n",
      "Train loss:  0.197030058018\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "print(weights)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = np.dot(x, weights)\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - sigmoid(output)\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error * sigmoid_prime(output)\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += 0\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
